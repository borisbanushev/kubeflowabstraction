{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "import datetime\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.common import write_numpy_to_dense_tensor\n",
    "import io\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://storage.googleapis.com/ml-pipeline/release/0.1.29/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel to pick up pip installed libraries\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Enter your S3 bucket below. you can leave it empty and a new bucket will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Make sure the role assumed by SageMaker either has access to the bucket, or has permissions to create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Please do NOT edit anything in the following cell !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your S3 bucket is: okjwlfrlpdsixvqq5128298611399215-kubeflow-pipeline-data\n",
      "Your region is: us-east-1\n",
      "Your account ID is: 231470146047\n",
      "Your SageMaker role ARN is: arn:aws:iam::231470146047:role/TeamRole\n"
     ]
    }
   ],
   "source": [
    "SAGEMAKER_ROLE_ARN=get_execution_role()\n",
    "AWS_ACCOUNT_ID=boto3.client('sts').get_caller_identity().get('Account')\n",
    "AWS_REGION = boto3.session.Session().region_name\n",
    "SAGEMAKER_ROLE_ARN='arn:aws:iam::{}:role/TeamRole'.format(AWS_ACCOUNT_ID)\n",
    "\n",
    "if not S3_BUCKET:\n",
    "    HASH = ''.join([random.choice(string.ascii_lowercase) for n in range(16)] + [random.choice(string.digits) for n in range(16)])\n",
    "    S3_BUCKET = '{}-kubeflow-pipeline-data'.format(HASH)\n",
    "\n",
    "print(f'Your S3 bucket is: {S3_BUCKET}')\n",
    "print(f'Your region is: {AWS_REGION}')\n",
    "print(f'Your account ID is: {AWS_ACCOUNT_ID}')\n",
    "print(f'Your SageMaker role ARN is: {SAGEMAKER_ROLE_ARN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Check if the bucket exits. Create it if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\n",
      "make_bucket: elvupwxbctfruhrb3403662892282909-kubeflow-pipeline-data\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://$S3_BUCKET || aws s3 mb s3://$S3_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Of course you can use your own data set instead of downloading it again. If so, **please make sure you follow the S3 structure and the data format requirements**. Link to the SageMaker built-in kmeans library - https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data will be uploaded to: s3://elvupwxbctfruhrb3403662892282909-kubeflow-pipeline-data/mnist_kmeans_example/train_data\n",
      "Test data will be uploaded to: s3://elvupwxbctfruhrb3403662892282909-kubeflow-pipeline-data/mnist_kmeans_example/test_data\n"
     ]
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "bucket = S3_BUCKET\n",
    "prefix = 'mnist_kmeans_example'\n",
    "\n",
    "\n",
    "train_data_key = f'{prefix}/train_data'\n",
    "test_data_key = f'{prefix}/test_data'\n",
    "train_data_location = 's3://{}/{}'.format(S3_BUCKET, train_data_key)\n",
    "test_data_location = 's3://{}/{}'.format(S3_BUCKET, test_data_key)\n",
    "print('Training data will be uploaded to: {}'.format(train_data_location))\n",
    "print('Test data will be uploaded to: {}'.format(test_data_location))\n",
    "\n",
    "# Convert the training data into the format required by the SageMaker KMeans algorithm\n",
    "buf = io.BytesIO()\n",
    "write_numpy_to_dense_tensor(buf, train_set[0], train_set[1])\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(S3_BUCKET).Object(train_data_key).upload_fileobj(buf)\n",
    "\n",
    "# Convert the test data into the format required by the SageMaker KMeans algorithm\n",
    "write_numpy_to_dense_tensor(buf, test_set[0], test_set[1])\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(S3_BUCKET).Object(test_data_key).upload_fileobj(buf)\n",
    "\n",
    "# Convert the valid data into the format required by the SageMaker KMeans algorithm\n",
    "numpy.savetxt('valid-data.csv', valid_set[0], delimiter=',', fmt='%g')\n",
    "s3_client = boto3.client('s3')\n",
    "input_key = \"{}/input/valid_data.csv\".format(prefix)\n",
    "s3_client.upload_file('valid-data.csv', S3_BUCKET, input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Full version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version gives you full control to absolutely every part of the pipeline, including:\n",
    "- what components to add,\n",
    "- detailed hyperparameter setting,\n",
    "- details on instance type,\n",
    "- etc.\n",
    "\n",
    "*If you want to use the very abstracted version - please, proceed to Step 3.2. below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the SageMaker components for Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp.aws import use_aws_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/deploy/component.yaml')\n",
    "sagemaker_batch_transform_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/batch_transform/component.yaml')\n",
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/hyperparameter_tuning/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://elvupwxbctfruhrb3403662892282909-kubeflow-pipeline-data/mnist_kmeans_example\n"
     ]
    }
   ],
   "source": [
    "# Configure S3 data path\n",
    "S3_PIPELINE_PATH='s3://{}/{}'.format(S3_BUCKET, prefix)\n",
    "print(S3_PIPELINE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='MNIST Classification pipeline',\n",
    "    description='MNIST Classification using KMEANS in SageMaker'\n",
    ")\n",
    "def mnist_classification(region='us-east-1',\n",
    "    image='382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1',\n",
    "    training_input_mode='File',\n",
    "    hpo_strategy='Bayesian',\n",
    "    hpo_metric_name='test:msd',\n",
    "    hpo_metric_type='Minimize',\n",
    "    hpo_early_stopping_type='Off',\n",
    "    hpo_static_parameters='{\"k\": \"10\", \"feature_dim\": \"784\"}',\n",
    "    hpo_integer_parameters='[{\"Name\": \"mini_batch_size\", \"MinValue\": \"500\", \"MaxValue\": \"600\"}, {\"Name\": \"extra_center_factor\", \"MinValue\": \"10\", \"MaxValue\": \"20\"}]',\n",
    "    hpo_continuous_parameters='[]',\n",
    "    hpo_categorical_parameters='[{\"Name\": \"init_method\", \"Values\": [\"random\", \"kmeans++\"]}]',\n",
    "    hpo_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}, \\\n",
    "               {\"ChannelName\": \"test\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/test_data\", \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    hpo_spot_instance='False',\n",
    "    hpo_max_wait_time='3600',\n",
    "    hpo_checkpoint_config='{}',\n",
    "    output_location=S3_PIPELINE_PATH + '/output',\n",
    "    output_encryption_key='',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count='1',\n",
    "    volume_size='50',\n",
    "    hpo_max_num_jobs='9',\n",
    "    hpo_max_parallel_jobs='2',\n",
    "    max_run_time='3600',\n",
    "    endpoint_url='',\n",
    "    network_isolation='True',\n",
    "    traffic_encryption='False',\n",
    "    train_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    train_spot_instance='False',\n",
    "    train_max_wait_time='3600',\n",
    "    train_checkpoint_config='{}',\n",
    "    batch_transform_instance_type='ml.m4.xlarge',\n",
    "    batch_transform_input=S3_PIPELINE_PATH + '/input',\n",
    "    batch_transform_data_type='S3Prefix',\n",
    "    batch_transform_content_type='text/csv',\n",
    "    batch_transform_compression_type='None',\n",
    "    batch_transform_ouput=S3_PIPELINE_PATH + '/output',\n",
    "    batch_transform_max_concurrent='4',\n",
    "    batch_transform_max_payload='6',\n",
    "    batch_strategy='MultiRecord',\n",
    "    batch_transform_split_type='Line',\n",
    "    role_arn=SAGEMAKER_ROLE_ARN\n",
    "    ):\n",
    "\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy=hpo_strategy,\n",
    "        metric_name=hpo_metric_name,\n",
    "        metric_type=hpo_metric_type,\n",
    "        early_stopping_type=hpo_early_stopping_type,\n",
    "        static_parameters=hpo_static_parameters,\n",
    "        integer_parameters=hpo_integer_parameters,\n",
    "        continuous_parameters=hpo_continuous_parameters,\n",
    "        categorical_parameters=hpo_categorical_parameters,\n",
    "        channels=hpo_channels,\n",
    "        output_location=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs=hpo_max_num_jobs,\n",
    "        max_parallel_jobs=hpo_max_parallel_jobs,\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=hpo_spot_instance,\n",
    "        max_wait_time=hpo_max_wait_time,\n",
    "        checkpoint_config=hpo_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=hpo.outputs['best_hyperparameters'],\n",
    "        channels=train_channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=train_spot_instance,\n",
    "        max_wait_time=train_max_wait_time,\n",
    "        checkpoint_config=train_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=training.outputs['training_image'],\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role_arn\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    if False:\n",
    "        prediction = sagemaker_deploy_op(\n",
    "            region=region,\n",
    "            endpoint_url=endpoint_url,\n",
    "            model_name_1=create_model.output,\n",
    "        ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    \n",
    "    if False:\n",
    "        batch_transform = sagemaker_batch_transform_op(\n",
    "            region=region,\n",
    "            endpoint_url=endpoint_url,\n",
    "            model_name=create_model.output,\n",
    "            instance_type=batch_transform_instance_type,\n",
    "            instance_count=instance_count,\n",
    "            max_concurrent=batch_transform_max_concurrent,\n",
    "            max_payload=batch_transform_max_payload,\n",
    "            batch_strategy=batch_strategy,\n",
    "            input_location=batch_transform_input,\n",
    "            data_type=batch_transform_data_type,\n",
    "            content_type=batch_transform_content_type,\n",
    "            split_type=batch_transform_split_type,\n",
    "            compression_type=batch_transform_compression_type,\n",
    "            output_location=batch_transform_ouput\n",
    "        ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(mnist_classification, 'mnist-classification-pipeline-v2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist-classification-pipeline-20200816140706\n"
     ]
    }
   ],
   "source": [
    "kf_run_name = f'mnist-classification-pipeline-{datetime.datetime.now():%Y%m%d%H%M%S}'\n",
    "print(kf_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/99200772-3c3a-45c7-a153-17fe48f902bf\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/25be5e9b-1962-4598-bd1b-64bbac782fe0\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "aws_experiment = client.create_experiment(name='aws')\n",
    "my_run = client.run_pipeline(aws_experiment.id, kf_run_name, 'mnist-classification-pipeline-v2.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Abstracted version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, data scientists only need to add a few parameters specific to what they want to do, such as whether of not to add hyperparameter training job, path to S3 bucket, etc, and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfabstraction import create_abstracted_kf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_HPO_runs = 10\n",
    "deploy_model = False\n",
    "add_batch_transform = False\n",
    "add_HPO = False\n",
    "S3_BUCKET = 'elvupwxbctfruhrb3403662892282909-kubeflow-pipeline-data'\n",
    "prefix = 'mnist_kmeans_example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/kfp/components/_data_passing.py:94: UserWarning: Missing type name was inferred as \"JsonObject\" based on the value \"{'k': '10', 'init_method': 'random', 'feature_dim': '784', 'extra_center_factor': '16', 'mini_batch_size': '516'}\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the run is: mnist-classification-pipeline-20200816162007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/99200772-3c3a-45c7-a153-17fe48f902bf\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/0693b776-8ce7-4740-aad3-7de6e32959d7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_abstracted_kf_pipeline(add_HPO, add_batch_transform, \\\n",
    "                              number_of_HPO_runs, deploy_model, f's3://{S3_BUCKET}/{prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
